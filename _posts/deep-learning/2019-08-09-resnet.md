---
layout: post
title: ResNet에 대하여
excerpt: "Deep Learning"
tags: [deep learning, programming, dl]
permalink: /deep-learning/:year/:month/:day/:title/
category : 딥러닝
---

## ResNet
2015년에 나왔고 여러 대회에서 놀라운 성적으로 우승한 네트워크이다.

Residual network의 줄임말이다. **residual learning** 이란 무엇일까? 
residual은 '남아 있는' 정도의 뜻으로 쓰이는데, '계산이 설명되지 않는' 등의 뜻 또한 갖고 있다.

딥러닝은 기본적으로 망이 깊어지면 성능이 더 좋아진다고 생각한다. VGGNet에서는 19-layer 까지 테스트를 하였는데,만약 19-layer보다 더 망을 깊게 설계하면 어떻게 될까?

![image](https://wiki.tum.de/download/attachments/22578294/Figure%201.bmp?version=1&modificationDate=1485208088253&api=v2)

그림에서 보는 것과 같이 56-레이어의 결과가 더 나쁘게 나왔다. 그 이유는  
망이 깊어지면 gradient vanishing/exploding 또는 degradation이 발생하게 된다. 하지만 ResNet은 망의 깊이를 1001-레이어까지 늘려서 설계했다.

### Skip Connection
평범한 CNN 망은 입력을 받아 weighted layer를 거쳐 출력을 낸다. 
ResNet은 layer의 입력을 layer의 출력에 바로 연결시키는 skip connection을 사용했다.

![image](https://www.oreilly.com/library/view/deep-learning-for/9781788295628/assets/3ae14008-5648-4502-ab17-49f7ad167dda.png)

Residual Learning의 중점은 degradation 문제를 해결하는 것이고 F(x) + x를 H(x)에 근사하게 한다. 

![image](https://2.bp.blogspot.com/-J7j0eUkNJ-w/WPU7iXKs6II/AAAAAAAAJt0/FMY6pOVMu34y2TziHnVPbadohKim1XpKACLcB/s1600/resnet_vs_plainnet.png)

모든 레이어를 거치는 것이 아니라 일부 레이어만 거치는 것이 핵심이다.  
이것을 fast net 이라고도 한다. 여러 레이어를 건너 뛰어서 목표에 빨리 도착할 수 있기 때문이다. 건너뛰는 갯수를 난수로 처리하면 반복할 때마다 다른 결과를 얻어 균형 잡힌 결과를 구할 수 있게 된다.  

